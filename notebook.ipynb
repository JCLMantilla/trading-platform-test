{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3baf175e",
   "metadata": {},
   "source": [
    "# Benchmarking of prototype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc0860",
   "metadata": {},
   "source": [
    "We are going to use the test data and feed it into Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fbf4abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "import time\n",
    "import httpx\n",
    "\n",
    "\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b33cffb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/13/z6c3b3d50nnbcf08brkd87dw0000gn/T/ipykernel_69529/1940412412.py:10: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator(\"datetime_est\", pre=True, always=True)\n"
     ]
    }
   ],
   "source": [
    "class Record(BaseModel):\n",
    "    input_report: str\n",
    "    output_report: str\n",
    "    datetime_est: Optional[str] = None\n",
    "    close: Optional[float] = None\n",
    "    signal: Optional[int] = None\n",
    "    conviction: Optional[float] = None\n",
    "    causality: Optional[str] = None\n",
    "\n",
    "    @validator(\"datetime_est\", pre=True, always=True)\n",
    "    def validate_datetime_format(cls, v):\n",
    "        if v is None:\n",
    "            return None\n",
    "        try:\n",
    "            # Parse datetime to enforce format\n",
    "            dt = datetime.strptime(v, \"%Y-%m-%d %H:%M:%S\")\n",
    "            return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        except ValueError:\n",
    "            raise ValueError(\"datetime_est must be in 'YYYY-MM-DD HH:MM:SS' Format!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4fed32cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pq.read_table(\"df_events.parquet\")\n",
    "records = [dict(zip(table.column_names, row)) for row in zip(*table.to_pydict().values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a56839",
   "metadata": {},
   "source": [
    "\n",
    "## Redis Insert latency benchmark:\n",
    "\n",
    "\n",
    "Checking Prometheus metrics whe have that using the query \n",
    "\n",
    "`histogram_quantile(0.95, rate(redis_insert_duration_seconds_bucket{operation=\"astore\"}[20h]))` \n",
    "we can get the p95 latency:\n",
    "\n",
    "```bash\n",
    "{instance=\"trading-api-container:8005\", job=\"trading-platform\", operation=\"astore\"}\t            0.7463349422954485\n",
    "```\n",
    "\n",
    "which is about 0.75 seconds for entries in the full dataset. And given that we embedded + inserted all records in about 5-10s each time, we can say that our program handles multiple inserting request in a decent way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b48d55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:44 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:45 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n",
      "21:09:46 httpx INFO   HTTP Request: POST http://localhost:8006/insert_data \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "async def post_record(client, url, record_dict):\n",
    "    response = await client.post(url, json=record_dict)\n",
    "    response_json = response.json()\n",
    "    #print(response_json['response'])\n",
    "\n",
    "async def main():\n",
    "    url = \"http://localhost:8006/insert_data\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = []\n",
    "        for _record in records[:50]: # Only first 50 records for testing\n",
    "            await asyncio.sleep(0.2)\n",
    "            record = _record.copy()\n",
    "            record['datetime_est'] = str(record['datetime_est']) # Enforce string type since in the dataset they are in pd.timestamp type\n",
    "            final_record = Record(**record)\n",
    "            tasks.append(post_record(client, url, final_record.model_dump()))\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6bd262",
   "metadata": {},
   "source": [
    "# Redis semantic caching with the `/decide` endpoint (30 requests)\n",
    "\n",
    "\n",
    "Now we are going to send concurrent request to check metrics of the caching of prompts against LLM responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f124206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:11 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n",
      "21:10:12 httpx INFO   HTTP Request: POST http://localhost:8006/decide?manual_embedding=true \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "async def post_record(client, url, record_dict):\n",
    "    response = await client.post(url, json=record_dict, params={\"manual_embedding\": True})\n",
    "    response_json = response.json()\n",
    "\n",
    "\n",
    "# The same way as before, we just change the payload and send the prompt in this schema: {\"prompt\": \"string\"}\n",
    "async def main():\n",
    "    url = \"http://localhost:8006/decide\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = []\n",
    "        for _record in records[:30]:\n",
    "            await asyncio.sleep(0.5)\n",
    "            record = _record.copy()\n",
    "            tasks.append(post_record(client, url, {\"prompt\": record.get(\"input_report\")}))\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e9a81",
   "metadata": {},
   "source": [
    "## After inserting 50 recods and performing a semantic cache with 30 records we have the following benchmarks:\n",
    "\n",
    "\n",
    "\n",
    "### Average embedding duration: \n",
    "\n",
    "`openai_embedding_duration_seconds_sum / openai_embedding_duration_seconds_count`\n",
    "\n",
    "```Python\n",
    "{instance=\"trading-api-container:8005\", job=\"trading-platform\", operation=\"embedding\"}  \t0.47188677787780764\n",
    "```\n",
    "\n",
    "### p0.95 latency for embeddings: \n",
    "\n",
    "`histogram_quantile(0.95,openai_embedding_duration_seconds_bucket)`\n",
    "\n",
    "```Python\n",
    "{instance=\"trading-api-container:8005\", job=\"trading-platform\", operation=\"embedding\"}\t    0.7187499999999999\n",
    "```\n",
    "\n",
    "### Average latency for semantic cache match: \n",
    "\n",
    "`semantic_cache_duration_seconds_sum / semantic_cache_duration_seconds_count`\n",
    "\n",
    "```Python\n",
    "{instance=\"trading-api-container:8005\", job=\"trading-platform\", operation=\"acheck\"}\t        0.004074819882710775\n",
    "```\n",
    "### p0.95 latency for semantic cache: \n",
    "\n",
    "`histogram_quantile(0.95, semantic_cache_duration_seconds_bucket)` \n",
    "\n",
    "```Python\n",
    "{instance=\"trading-api-container:8005\", job=\"trading-platform\", operation=\"acheck\"}\t        0.009375\n",
    "\n",
    "```\n",
    "\n",
    "## Now lets dive into the actual benchmarks of the endpoint:\n",
    "\n",
    "### Average latency: \n",
    "\n",
    "`http_request_duration_seconds_sum{endpoint=\"/decide\"} / http_request_duration_seconds_count{endpoint=\"/decide\"}`\n",
    "\n",
    "```Python\n",
    "{endpoint=\"/decide\", instance=\"trading-api-container:8005\", job=\"trading-platform\", method=\"POST\"}\t    0.4941580216089884\n",
    "\n",
    "```\n",
    "\n",
    "### p0.95 Latency: \n",
    "\n",
    "`histogram_quantile(0.95, http_request_duration_seconds_bucket{endpoint=\"/decide\"})`\n",
    "\n",
    "```Python\n",
    "{endpoint=\"/decide\", instance=\"trading-api-container:8005\", job=\"trading-platform\", method=\"POST\"}\t0.7249999999999999\n",
    "```\n",
    "\n",
    "\n",
    "# *Core bottleneck is embedding time for full prompts!*\n",
    "\n",
    "\n",
    "# Then, how can we improve this embedding speed? \n",
    "\n",
    "There are two solutions aproaches:\n",
    "\n",
    "1. Using a higher speed embedder: We can use simpler or smaller embedding models, that comes at risks of losing embedding quality that might capture important information. Keep in mind that I already swiched Open AI's embedding model from large to small, which reduced p0.95 latency of the full /decide request by around 0.25s (Using test dataset), which is a significant improvement on the context of what we are trying to achieve.\n",
    "\n",
    "2. Pre-embed all heavy/static prompts (instruction prompts, prior analyzed news, etc), keep them in a traditional cache to store their vector representation so each time we require their embedding we can perform an extremely fast look up. At runtime, whenever an incoming news chunk arrives to the system, we quickly embed it using the model of our choice, then, try recent news + prompts combinations and compose their vectors via sum+norm to approximate the combined prompt+context embedding for cache lookup. Then we can perform a semantic cache lookup in order to attempt to retrieve a real trading decision if the similarity for one of the composed vectors have a similarity > than a predefined threshhold Ï„.\n",
    "\n",
    "3. Mixed aproach; We can try the first two approaches at the same time, by trying different embedding models!\n",
    "\n",
    "\n",
    "\n",
    "We could do the following:\n",
    "\n",
    " - Have all prompts + chunks of analized news in a regular cache, lets call it Embedding cache (EC). Refer to https://redis.io/docs/latest/develop/ai/redisvl/user_guide/embeddings_cache/\n",
    "\n",
    " - Have in the semantic cache (sc) the sum of those chunks and prompts normalized together with the response containing the signal and metadata\n",
    "\n",
    " - When a new chunk of news comes in (which is significatively smaller than a full prompt), we quickly embed it and add it into our pre-embedded news + prompts cache.\n",
    "\n",
    " - Then we try to append the incoming news into different pre-embedded chunks + news combinations. We approximate the full-prompt vecor embedding using a vector compositions of every good candidate for a hit in the SC.\n",
    "\n",
    "- I could not integrate this system described above due to tight deadlines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54358aad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
